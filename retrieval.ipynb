{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stuff from arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "client = arxiv.Client()\n",
    "\n",
    "def get_pdf(input: str, output_path): \n",
    "    \"\"\" Accepts user input and downloads relevant pdf. \n",
    "        - validate input.. \n",
    "        - Currently only accepts arXiv id, but can expand search query functionality. \n",
    "        https://info.arxiv.org/help/api/user-manual.html#_details_of_atom_results_returned. \n",
    "        - if user provides url to pdf, can directly call read_pdf with the url.. \n",
    "        - get multiple papers?\n",
    "        - output to cloud.. \n",
    "        - generate (somewhere in code) f string with filename \n",
    "          for easy storage (and later retrieval)\n",
    "\n",
    "    \"\"\"\n",
    "    paper = next(arxiv.Client().results(arxiv.Search(id_list=[input])))\n",
    "    paper.download_pdf(dirpath = output_path)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pdf(input = '1706.03762', output_path='retrieved_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the arXiv API : use python to retrieve \n",
    "import urllib, urllib.request\n",
    "url = 'https://arxiv.org/abs/1705.02315'\n",
    "data = urllib.request.urlopen(url)\n",
    "print(data.read().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pypi.org/project/arxiv/\n",
    "\n",
    "import arxiv\n",
    "client = arxiv.Client()\n",
    "search_by_id = arxiv.Search(id_list = ['2309.11838'])\n",
    "sample = next(client.results(search_by_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retrieved_data/main_doc.pdf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample, sample.title\n",
    "sample.download_pdf(filename='main_doc.pdf', dirpath = 'retrieved_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = arxiv.Search(\n",
    "    query= 'Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and \\\n",
    "        Percy Liang. 2016. SQuAD: 100,000+ questions for\\\n",
    "        machine comprehension of text. In Proceedings of\\\n",
    "        the 2016 Conference on Empirical Methods in Natu-\\\n",
    "        ral Language Processing, pages 2383â€“2392, Austin,\\\n",
    "        Texas. Association for Computational Linguistics.', \n",
    "    max_results = 3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retrieved_data/doc1.pdf'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(client.results(search)).download_pdf(filename='doc1.pdf', dirpath='retrieved_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "def read_pdf(filepath): \n",
    "    ''' Uses PyMuPDFReader to parse PDF and return Document Object.\n",
    "    '''\n",
    "\n",
    "    loader = PyMuPDFReader()\n",
    "    doc = loader.load(filepath)\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'retrieved_data/1706.03762v7.Attention_Is_All_You_Need.pdf'\n",
    "doc = read_pdf(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsherpa.readers import LayoutPDFReader\n",
    "\n",
    "def read_pdf_old(filepath): \n",
    "    \"\"\" Uses LLMSherpa API to parse PDF. Returns a llama-index Document.\n",
    "    \n",
    "    Can later do it locally with their public docker image.\n",
    "    https://www.reddit.com/r/LocalLLaMA/comments/18lwa5c/alternative_tool_like_llmsherpa_that_can_run/\n",
    "    \"\"\"\n",
    "    # This doc does not work with the node parser. \n",
    "    \n",
    "    llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "    pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "    doc = pdf_reader.read_pdf(filepath)\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_doc = read_pdf_old(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trial and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trials\n",
    "\n",
    "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "pdf_url = 'retrieved_data/main_doc.pdf'\n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "doc = pdf_reader.read_pdf(pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Does this work with direct url to arxiv?\n",
    "\n",
    "new_pdf_url = 'https://arxiv.org/pdf/2110.04770.pdf'\n",
    "doc2 = pdf_reader.read_pdf(new_pdf_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://ambikasukla.substack.com/p/efficient-rag-with-document-layout?r=ft8uc&utm_campaign=post&utm_medium=web&triedRedirect=true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llmsherpa.readers.layout_reader.Document"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2.to_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arxiv_references(filepath) -> set:\n",
    "    \"\"\" Returns set of all arXiv ids for references in the paper.\n",
    "        - Uses LayoutPDF reader, so \n",
    "        Necessarily limited to references that exist in arXiv.\n",
    "    \"\"\"\n",
    "\n",
    "    def read_pdf_old(filepath): \n",
    "        \"\"\" Uses LLMSherpa API to parse PDF. Returns a llama-index Document.\n",
    "        \n",
    "        Can later do it locally with their public docker image.\n",
    "        https://www.reddit.com/r/LocalLLaMA/comments/18lwa5c/alternative_tool_like_llmsherpa_that_can_run/\n",
    "        \"\"\"\n",
    "        # This doc does not work with the node parser. \n",
    "        llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "        pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "        doc = pdf_reader.read_pdf(filepath)\n",
    "        return doc\n",
    "\n",
    "    doc = read_pdf_old(filepath)\n",
    "    patterns = [r\"\\w+-\\w+/\\d{7}\", r\"\\d{4}\\.\\d{4,5}\"]\n",
    "\n",
    "    refs = set()\n",
    "    for pattern in patterns:\n",
    "        [refs.add(item) for item in re.findall(pattern, doc.to_text())]\n",
    "    return refs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = get_arxiv_references(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_arxiv_references(doc)-> set:\n",
    "     \"\"\" Returns set of all arXiv ids for references in the paper.\n",
    "     Uses LayoutPDFReader object. \n",
    "\n",
    "     Necessarily limited to references that exist in arXiv. \n",
    "     \"\"\"\n",
    "     patterns = ['\\w+-\\w+/\\d{7}', '\\d{4}\\.\\d{4,5}']\n",
    "     \n",
    "     refs = set()\n",
    "     for pattern in patterns: \n",
    "          [refs.add(item) for item in re.findall(pattern, doc.to_text())]\n",
    "     return refs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = get_arxiv_references(test_doc)\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trial and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trials \n",
    "\n",
    "import re\n",
    "test_doc.to_text()\n",
    "\n",
    "\n",
    "refs = [ref[1] for ref in re.findall('(arXiv:)*(\\d{4}\\.\\d{4,5})', test_doc.to_text())]\n",
    "refs = set(refs)\n",
    "len(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs = set()\n",
    "[refs.add(item) for item in re.findall('\\d{4}\\.\\d{4,5}', test_doc.to_text())]\n",
    "len(refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "def build_vector_store(doc): \n",
    "    ''' Builds in-memory vector store using a document.\n",
    "        - uses OpenAI embedding atm\n",
    "    '''\n",
    "    node_parser = SentenceSplitter(chunk_size = 512, chunk_overlap = 50)\n",
    "    nodes = node_parser.get_nodes_from_documents(doc)   \n",
    "\n",
    "    embed_model = embed_model()\n",
    "\n",
    "    for node in nodes: \n",
    "        node_embedding = embed_model.get_text_embedding(\n",
    "            node.get_content(metadata_mode= 'all')\n",
    "            )\n",
    "        node.embedding = node_embedding\n",
    "\n",
    "    index = VectorStoreIndex(nodes)\n",
    "    return index\n",
    "\n",
    "    def embed_model():\n",
    "        import os\n",
    "        OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "        return OpenAIEmbedding()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "doc = loader.load(file_path='/Users/sonal/Documents/MSDS/spring2/entrepreneurship/Podsicle/retrieved_data/1706.03762v7.Attention_Is_All_You_Need.pdf')\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size = 512, chunk_overlap = 50)\n",
    "nodes = node_parser.get_nodes_from_documents(doc)\n",
    "\n",
    "# Generate Embeddings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "for node in nodes: \n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode= 'all')\n",
    "        )\n",
    "    node.embedding = node_embedding\n",
    "\n",
    "index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(llama_index.core.indices.vector_store.base.VectorStoreIndex, list)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(index), type(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vector_store', 'doc_store', 'index_store', 'graph_store'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.storage_context.to_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't seem to get the key from the environment variables\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"retrieved_data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"explain the paper in 100 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [https://docs.llamaindex.ai/en/stable/examples/low_level/vector_store/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install llama-index-readers-file pymupdf\n",
    "\n",
    "# Load doc\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "doc = loader.load(file_path='/Users/sonal/Documents/MSDS/spring2/entrepreneurship/Podsicle/retrieved_data/1706.03762v7.Attention_Is_All_You_Need.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse into Nodes\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size = 512, chunk_overlap = 50)\n",
    "nodes = node_parser.get_nodes_from_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Embeddings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding()\n",
    "for node in nodes: \n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode= 'all')\n",
    "        )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "index = VectorStoreIndex.from_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.vector_stores.simple.SimpleVectorStore at 0x111c67250>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.vector_store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

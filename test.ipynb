{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bassime/Desktop/fullstack/Podsicle/Bassim/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = GeminiText()\n",
    "text_model.generate_completion('What is 5 + 5?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bassime/Desktop/fullstack/Podsicle/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from podcast_maker import PodcastMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['What inspired the development of a network architecture based solely on attention mechanisms?',\n",
       "  'How does the proposed Transformer architecture differ from traditional sequence transduction models?',\n",
       "  'What advantages do the Transformer models offer in terms of quality, parallelizability, and training time compared to existing models?',\n",
       "  'Can you elaborate on the experimental results that demonstrate the superiority of the Transformer models in machine translation tasks?',\n",
       "  'How does the Transformer model generalize to other tasks, as mentioned in the abstract?'],\n",
       " ['The development of a network architecture based solely on attention mechanisms was inspired by the need to create a simpler model that could dispense with recurrence and convolutions entirely, while still achieving superior quality in machine translation tasks.',\n",
       "  'The proposed Transformer architecture differs from traditional sequence transduction models by being based solely on attention mechanisms, eliminating the need for recurrence and convolutions entirely.',\n",
       "  'The Transformer models offer superior quality, increased parallelizability, and reduced training time compared to existing models.',\n",
       "  'The experimental results demonstrate the superiority of the Transformer models in machine translation tasks by achieving a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, surpassing existing best results by over 2 BLEU. Additionally, on the WMT 2014 English-to-French translation task, the Transformer model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, which is a fraction of the training costs of the best models from the literature.',\n",
       "  'The Transformer model generalizes well to other tasks by successfully applying it to English constituency parsing with both large and limited training data.'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podcast_maker import PodcastMaker\n",
    "podcast_maker = PodcastMaker(url=\"https://arxiv.org/abs/1706.03762\")\n",
    "\n",
    "# Generate interview questions\n",
    "podcast_maker.generate_podcast_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What inspired the development of a network architecture based solely on attention mechanisms?',\n",
       " 'How does the proposed Transformer architecture differ from traditional sequence transduction models?',\n",
       " 'What advantages does the Transformer model offer in terms of quality, parallelizability, and training time compared to existing models?',\n",
       " 'Can you elaborate on the experimental results that demonstrate the superiority of the Transformer model in machine translation tasks?',\n",
       " 'How does the Transformer model generalize to other tasks, as mentioned in the abstract?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.response.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

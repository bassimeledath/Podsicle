{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/Podsicle/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = GeminiText()\n",
    "intro = text_model.generate_completion(\"5+5\")\n",
    "intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['What inspired the development of a network architecture based solely on attention mechanisms?',\n",
       "  'How does the proposed Transformer architecture differ from traditional sequence transduction models?',\n",
       "  'Can you elaborate on the advantages of the Transformer model in terms of quality, parallelizability, and training time compared to existing models?',\n",
       "  'What were the key findings from the experiments on machine translation tasks regarding the performance of the Transformer model?',\n",
       "  'How does the Transformer model demonstrate its generalizability to other tasks, as mentioned in the abstract?'],\n",
       " ['The development of a network architecture based solely on attention mechanisms was inspired by the need to create a simpler model that could improve quality, be more parallelizable, and require less training time compared to existing complex recurrent or convolutional neural network models.',\n",
       "  'The proposed Transformer architecture differs from traditional sequence transduction models by being based solely on attention mechanisms, eliminating the need for recurrence and convolutions entirely.',\n",
       "  'The Transformer model offers superior quality in terms of translation performance, as evidenced by achieving higher BLEU scores on machine translation tasks compared to existing models. Additionally, the Transformer model is more parallelizable, allowing for faster training times and efficient utilization of computational resources. This model requires significantly less time to train while maintaining high translation quality, making it a more efficient and effective option compared to traditional models that rely on recurrent or convolutional neural networks.',\n",
       "  'The key findings from the experiments on machine translation tasks regarding the performance of the Transformer model were its superior quality, high parallelizability, and significantly reduced training time compared to existing models. The Transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, surpassing the existing best results by over 2 BLEU. Additionally, on the WMT 2014 English-to-French translation task, the Transformer model achieved a state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, demonstrating its effectiveness and efficiency in translation tasks.',\n",
       "  'The Transformer model demonstrates its generalizability to other tasks by successfully applying it to English constituency parsing with both large and limited training data.'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from podcast_maker import PodcastMaker\n",
    "podcast_maker = PodcastMaker(url=\"https://arxiv.org/abs/1706.03762\")\n",
    "\n",
    "# Generate interview questions\n",
    "questions_ans = podcast_maker.generate_podcast_text()\n",
    "questions_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from podcast_style import *\n",
    "\n",
    "podcast = PodcastStyler('Transformer model',\"https://arxiv.org/abs/1706.03762\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'host': [\"Welcome to Podsicle, the podcast that dives into the latest and greatest in technology. I'm your host, and today we're talking about Transformer models, a revolutionary new type of neural network that's taking the AI world by storm. I'm joined by an expert on the topic, who will help us understand what Transformer models are, how they work, and what their potential applications are. So sit back, relax, and get ready to learn about one of the most exciting developments in AI today.\",\n",
       "  \"Now, let's dive into the conversation. To kick things off, I'd like to ask you about the inspiration behind developing a network architecture based solely on attention mechanisms.\",\n",
       "  \"That's a fascinating insight into the motivations behind the development of the Transformer architecture. It's clear that the pursuit of simplicity and efficiency was a driving force in its creation. Now, let's delve deeper into the specifics of the Transformer architecture. How does it differ from traditional sequence transduction models? What are the key innovations that set it apart and contribute to its superior performance in machine translation tasks?\",\n",
       "  \"Absolutely, the Transformer architecture has revolutionized the field of machine translation with its unique approach. By eliminating recurrence and convolutions and relying solely on attention mechanisms, it has achieved remarkable results. Now, let's delve deeper into the specific advantages that Transformer models offer compared to existing models. How do they fare in terms of quality, parallelizability, and training time? Let's explore these aspects in detail.\",\n",
       "  \"Indeed, the Transformer models have brought about a paradigm shift in machine translation, offering a compelling alternative to traditional approaches. Their innovative architecture, characterized by the exclusive use of attention mechanisms, has yielded impressive results. As we continue our exploration of the Transformer models' capabilities, let's now turn our attention to their specific advantages over existing models. How do they compare in terms of translation quality, parallelizability, and training time? Let's delve into these aspects and uncover the strengths that make Transformer models stand out in the field of machine translation.\\n\\nCan you elaborate on the experimental results that demonstrate the superiority of the Transformer models in machine translation tasks? Provide specific examples and quantitative metrics to support your claims.\",\n",
       "  \"Certainly, the experimental results presented in the previous message provide compelling evidence of the Transformer models' superiority in machine translation tasks. The impressive BLEU scores achieved on both the WMT 2014 English-to-German and English-to-French translation tasks demonstrate the model's exceptional translation quality and efficiency. These quantitative metrics strongly support the claim that Transformer models offer a compelling alternative to traditional approaches in machine translation.\\n\\nAs a follow-up to our ongoing discussion, I would like to explore how the Transformer model generalizes to other tasks, as mentioned in the abstract. While the previous message focused on the model's performance in machine translation, the abstract suggests that Transformer models have broader applicability. I am particularly interested in understanding how the model's unique architecture and attention mechanism enable it to perform well on a variety of tasks beyond machine translation.\",\n",
       "  \"That concludes our show, thank you so much for joining us today. Your insights into the Transformer model's capabilities and its potential for broader applicability in natural language processing have been invaluable. The model's versatility and impressive performance in tasks beyond machine translation, such as English constituency parsing, demonstrate its potential to revolutionize various NLP domains. We appreciate your time and expertise, and we look forward to following the continued advancements of the Transformer model in the future.\",\n",
       "  \"Well, folks, that's all for this episode of Podsicle. Thank you so much for listening. I hope you enjoyed the show. If you did, please leave us a review on iTunes or wherever you listen to your podcasts. It really helps us out. And don't forget to subscribe so you can catch our next episode. Until then, stay cool and keep on podding!\"],\n",
       " 'guest': [\"Thank you for having me on the show. I'm excited to talk about Transformer models and their potential applications.\",\n",
       "  'The inspiration behind developing a network architecture based solely on attention mechanisms stemmed from the desire to create a simpler model. This model would dispense with recurrence and convolutions entirely, while still achieving superior quality in machine translation tasks. By focusing solely on attention mechanisms, we aimed to create a more efficient and effective model that could capture long-range dependencies and relationships within the data.',\n",
       "  'Indeed, the Transformer architecture stands out from traditional sequence transduction models by relying exclusively on attention mechanisms. This fundamental shift away from recurrence and convolutions has been a key factor in its success. The absence of these traditional components not only simplifies the architecture but also contributes to its efficiency and effectiveness in machine translation tasks.',\n",
       "  \"Certainly, the Transformer models have revolutionized machine translation with their unique approach. By eliminating recurrence and convolutions and relying solely on attention mechanisms, they have achieved remarkable results. Now, let's delve deeper into the specific advantages that Transformer models offer compared to existing models. How do they fare in terms of quality, parallelizability, and training time? Let's explore these aspects in detail.\",\n",
       "  \"Certainly, the experimental results provide compelling evidence of the Transformer models' superiority in machine translation tasks. For instance, on the WMT 2014 English-to-German translation task, the Transformer model achieved a BLEU score of 28.4, surpassing the best results, including ensembles, by over 2 BLEU. This remarkable achievement demonstrates the model's exceptional translation quality.\\n\\nFurthermore, on the WMT 2014 English-to-French translation task, the Transformer model set a new state-of-the-art BLEU score of 41.8 with just 3.5 days of training on eight GPUs. This result not only highlights the model's superior performance but also its impressive efficiency in terms of training time. These quantitative metrics provide strong support for the claim that Transformer models offer a compelling alternative to traditional approaches in machine translation.\",\n",
       "  \"Indeed, the Transformer model's capabilities extend beyond machine translation. Its unique architecture and attention mechanism allow it to generalize well to other tasks. One notable example is English constituency parsing, where the Transformer model has demonstrated impressive performance with both large and limited training data. This versatility highlights the model's potential for broader applicability in various natural language processing domains.\",\n",
       "  \"Thank you so much for having me on the show today. It's been a pleasure to share my thoughts on the Transformer model and its potential for broader applicability in natural language processing. I'm excited to see what the future holds for this powerful technology, and I'm grateful for the opportunity to have shared my insights with your listeners.\"]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = podcast.podcast_conv()\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output.mp3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_s = GoogleTTS()\n",
    "text_to_s.generate_completion('Whyyyyyy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transcript = {'host':['This is text 1 for host', 'This is text 2 for host', 'This is text 3 for host'],\n",
    "                   'guest':['This is text 1 for guest', 'This is text 2 for guest']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcript_to_speech import *\n",
    "\n",
    "generate_mp3(voice1=\"en-US-Standard-A\", voice2=\"en-US-Standard-D\",transcript=test_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
